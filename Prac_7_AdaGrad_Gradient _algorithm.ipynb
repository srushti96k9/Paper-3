{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaptive Gradient Algorithm (Adagrad) is an algorithm for gradient-based optimization. The learning rate is adapted component-wise to the parameters by incorporating knowledge of past observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0575262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0 f([-0.06595599  0.34064899]) = 0.12039\n",
      ">1 f([-0.02902286  0.27948766]) = 0.07896\n",
      ">2 f([-0.0129815   0.23463749]) = 0.05522\n",
      ">3 f([-0.00582483  0.1993997 ]) = 0.03979\n",
      ">4 f([-0.00261527  0.17071256]) = 0.02915\n",
      ">5 f([-0.00117437  0.14686138]) = 0.02157\n",
      ">6 f([-0.00052736  0.12676134]) = 0.01607\n",
      ">7 f([-0.00023681  0.10966762]) = 0.01203\n",
      ">8 f([-0.00010634  0.09503809]) = 0.00903\n",
      ">9 f([-4.77542704e-05  8.24607972e-02]) = 0.00680\n",
      ">10 f([-2.14444463e-05  7.16123835e-02]) = 0.00513\n",
      ">11 f([-9.62980437e-06  6.22327049e-02]) = 0.00387\n",
      ">12 f([-4.32434258e-06  5.41085063e-02]) = 0.00293\n",
      ">13 f([-1.94188148e-06  4.70624414e-02]) = 0.00221\n",
      ">14 f([-8.72017797e-07  4.09453989e-02]) = 0.00168\n",
      ">15 f([-3.91586740e-07  3.56309531e-02]) = 0.00127\n",
      ">16 f([-1.75845235e-07  3.10112252e-02]) = 0.00096\n",
      ">17 f([-7.89647442e-08  2.69937139e-02]) = 0.00073\n",
      ">18 f([-3.54597657e-08  2.34988084e-02]) = 0.00055\n",
      ">19 f([-1.59234984e-08  2.04577993e-02]) = 0.00042\n",
      ">20 f([-7.15057749e-09  1.78112581e-02]) = 0.00032\n",
      ">21 f([-3.21102543e-09  1.55077005e-02]) = 0.00024\n",
      ">22 f([-1.44193729e-09  1.35024688e-02]) = 0.00018\n",
      ">23 f([-6.47513760e-10  1.17567908e-02]) = 0.00014\n",
      ">24 f([-2.90771361e-10  1.02369798e-02]) = 0.00010\n",
      ">25 f([-1.30573263e-10  8.91375193e-03]) = 0.00008\n",
      ">26 f([-5.86349941e-11  7.76164047e-03]) = 0.00006\n",
      ">27 f([-2.63305247e-11  6.75849105e-03]) = 0.00005\n",
      ">28 f([-1.18239380e-11  5.88502652e-03]) = 0.00003\n",
      ">29 f([-5.30963626e-12  5.12447017e-03]) = 0.00003\n",
      ">30 f([-2.38433568e-12  4.46221948e-03]) = 0.00002\n",
      ">31 f([-1.07070548e-12  3.88556303e-03]) = 0.00002\n",
      ">32 f([-4.80809073e-13  3.38343471e-03]) = 0.00001\n",
      ">33 f([-2.15911255e-13  2.94620023e-03]) = 0.00001\n",
      ">34 f([-9.69567190e-14  2.56547145e-03]) = 0.00001\n",
      ">35 f([-4.35392094e-14  2.23394494e-03]) = 0.00000\n",
      ">36 f([-1.95516389e-14  1.94526160e-03]) = 0.00000\n",
      ">37 f([-8.77982370e-15  1.69388439e-03]) = 0.00000\n",
      ">38 f([-3.94265180e-15  1.47499203e-03]) = 0.00000\n",
      ">39 f([-1.77048011e-15  1.28438640e-03]) = 0.00000\n",
      ">40 f([-7.95048604e-16  1.11841198e-03]) = 0.00000\n",
      ">41 f([-3.57023093e-16  9.73885702e-04]) = 0.00000\n",
      ">42 f([-1.60324146e-16  8.48035867e-04]) = 0.00000\n",
      ">43 f([-7.19948720e-17  7.38448972e-04]) = 0.00000\n",
      ">44 f([-3.23298874e-17  6.43023418e-04]) = 0.00000\n",
      ">45 f([-1.45180009e-17  5.59929193e-04]) = 0.00000\n",
      ">46 f([-6.51942732e-18  4.87572776e-04]) = 0.00000\n",
      ">47 f([-2.92760228e-18  4.24566574e-04]) = 0.00000\n",
      ">48 f([-1.31466380e-18  3.69702307e-04]) = 0.00000\n",
      ">49 f([-5.90360555e-19  3.21927835e-04]) = 0.00000\n",
      "Done!\n",
      "f([-5.90360555e-19  3.21927835e-04]) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import asarray\n",
    "from numpy.random import rand, seed\n",
    "\n",
    "# objective function\n",
    "def objective(x, y):\n",
    "    return x**2.0 + y**2.0\n",
    "\n",
    "# derivative of the objective function\n",
    "def derivative(x, y):\n",
    "    return asarray([x * 2.0, y * 2.0])\n",
    "\n",
    "# gradient descent algorithm with Adagrad\n",
    "def adagrad(objective, derivative, bounds, n_iter, step_size):\n",
    "    seed(1)\n",
    "    # generate an initial point\n",
    "    solution = bounds[:, 0] + rand(len(bounds)) * (bounds[:, 1] - bounds[:, 0])\n",
    "    # list of the sum square gradients for each variable\n",
    "    sq_grad_sums = [0.0 for _ in range(bounds.shape[0])]\n",
    "    # run the gradient descent\n",
    "    for it in range(n_iter):\n",
    "        # calculate gradient\n",
    "        gradient = derivative(solution[0], solution[1])\n",
    "        # update the sum of the squared partial derivatives\n",
    "        for i in range(gradient.shape[0]):\n",
    "            sq_grad_sums[i] += gradient[i]**2.0\n",
    "        # build a solution one variable at a time\n",
    "        new_solution = list()\n",
    "        for i in range(solution.shape[0]):\n",
    "            # calculate the step size for this variable\n",
    "            alpha = step_size / (1e-8 + sqrt(sq_grad_sums[i]))\n",
    "            # calculate the new position in this variable\n",
    "            value = solution[i] - alpha * gradient[i]\n",
    "            # store this variable\n",
    "            new_solution.append(value)\n",
    "        # evaluate the candidate point\n",
    "        solution = asarray(new_solution)\n",
    "        solution_eval = objective(solution[0], solution[1])\n",
    "        # report progress\n",
    "        print('>%d f(%s) = %.5f' % (it, solution, solution_eval))\n",
    "    return [solution, solution_eval]\n",
    "\n",
    "# define range for input\n",
    "bounds = asarray([[-1.0, 1.0], [-1.0, 1.0]])\n",
    "# define the total iterations\n",
    "n_iter = 50\n",
    "# define the step size\n",
    "step_size = 0.1\n",
    "# perform the gradient descent search with Adagrad\n",
    "best, score = adagrad(objective, derivative, bounds, n_iter, step_size)\n",
    "print('Done!')\n",
    "print('f(%s) = %f' % (best, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a597c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
